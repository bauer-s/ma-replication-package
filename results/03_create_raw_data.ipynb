{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c0a542",
   "metadata": {},
   "source": [
    "# Aggregated CSV Generation (`raw_data/`)\n",
    "\n",
    "\n",
    "\n",
    "This notebook regenerates the aggregated CSV inputs used by the analysis notebooks in this folder.\n",
    "\n",
    "\n",
    "\n",
    "Inputs: raw experiment logs originally stored under `whisker_cluster_experiments/results/` (baseline + surrogate-model).\n",
    "\n",
    "For the replication package, **copy the contents of that folder into `../temp_results/`** at the repository root.\n",
    "\n",
    "\n",
    "\n",
    "Outputs: aggregated CSVs under `raw_data/` (relative to this `results/` folder).\n",
    "\n",
    "\n",
    "\n",
    "Run it from the `results/` folder (default VS Code notebook behavior).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed34c7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline path: /Users/stefan/Workspace/bauers-ma/thesis/temp/results/baseline\n",
      "Surrogate path: /Users/stefan/Workspace/bauers-ma/thesis/temp/results/surrogate-model\n",
      "Raw data path: /Users/stefan/Workspace/bauers-ma-replication-package/results/raw_data\n"
     ]
    }
   ],
   "source": [
    "# Imports and configuration\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# This notebook lives in temp/results_analysis_thesis/\n",
    "BASE_PATH = Path('.')\n",
    "RESULTS_DIR = Path('/Users/stefan/Workspace/bauers-ma/thesis/temp/results/')\n",
    "BASELINE_PATH = RESULTS_DIR / 'baseline'\n",
    "SURROGATE_PATH = RESULTS_DIR / 'surrogate-model'\n",
    "\n",
    "# Folder where all generated CSV files will be stored\n",
    "RAW_DATA_PATH = BASE_PATH / 'raw_data'\n",
    "RAW_DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "CONFIG_MAPPING = {\n",
    "    'genProg-surrogate-model-fi-00': 'FI-00',\n",
    "    'genProg-surrogate-model-fi-05': 'FI-50',\n",
    "    'genProg-surrogate-model-fi-07': 'FI-70',\n",
    "    'genProg-surrogate-model-to-05': 'TO-50',\n",
    "    'genProg-surrogate-model-to-07': 'TO-70',\n",
    "}\n",
    "\n",
    "CONFIG_ORDER = ['FI-00', 'FI-50', 'FI-70', 'TO-50', 'TO-70']\n",
    "\n",
    "if not BASELINE_PATH.exists():\n",
    "    raise FileNotFoundError(f'Baseline folder not found: {BASELINE_PATH}')\n",
    "if not SURROGATE_PATH.exists():\n",
    "    raise FileNotFoundError(f'Surrogate folder not found: {SURROGATE_PATH}')\n",
    "\n",
    "print(f'Baseline path: {BASELINE_PATH.resolve()}')\n",
    "print(f'Surrogate path: {SURROGATE_PATH.resolve()}')\n",
    "print(f'Raw data path: {RAW_DATA_PATH.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870ba154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tasks: 410\n",
      "Tasks per configuration:\n",
      "config_label\n",
      "FI-00    82\n",
      "FI-50    82\n",
      "FI-70    82\n",
      "TO-50    82\n",
      "TO-70    82\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load task metadata for baseline and surrogate-model runs\n",
    "baseline_tasks = pd.read_csv(BASELINE_PATH / 'tasks.csv')\n",
    "surrogate_tasks = pd.read_csv(SURROGATE_PATH / 'tasks.csv')\n",
    "\n",
    "baseline_tasks['config_label'] = baseline_tasks['config'].map(CONFIG_MAPPING)\n",
    "surrogate_tasks['config_label'] = surrogate_tasks['config'].map(CONFIG_MAPPING)\n",
    "\n",
    "all_tasks = pd.concat([baseline_tasks, surrogate_tasks], ignore_index=True)\n",
    "\n",
    "print(f'Total tasks: {len(all_tasks)}')\n",
    "print('Tasks per configuration:')\n",
    "print(all_tasks['config_label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db156ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to load and summarize a single run\n",
    "\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d effect size for two 1D numpy/pandas arrays.\"\"\"\n",
    "    group1 = pd.Series(group1).dropna()\n",
    "    group2 = pd.Series(group2).dropna()\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return np.nan\n",
    "    var1, var2 = group1.var(), group2.var()\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    if pooled_std <= 0:\n",
    "        return 0.0\n",
    "    return (group1.mean() - group2.mean()) / pooled_std\n",
    "\n",
    "def load_output_csv(job_id, base_path):\n",
    "    output_file = base_path / str(job_id) / 'output.csv'\n",
    "    if output_file.exists():\n",
    "        try:\n",
    "            return pd.read_csv(output_file)\n",
    "        except Exception as exc:\n",
    "            print(f'Error loading {output_file}: {exc}')\n",
    "    return None\n",
    "\n",
    "def load_timing_data(job_id, base_path):\n",
    "    started_file = base_path / str(job_id) / 'started.txt'\n",
    "    finished_file = base_path / str(job_id) / 'finished.txt'\n",
    "    timing = {}\n",
    "    try:\n",
    "        if started_file.exists():\n",
    "            started = started_file.read_text().strip()\n",
    "            timing['started'] = datetime.fromisoformat(started)\n",
    "        if finished_file.exists():\n",
    "            finished = finished_file.read_text().strip()\n",
    "            timing['finished'] = datetime.fromisoformat(finished)\n",
    "        if 'started' in timing and 'finished' in timing:\n",
    "            timing['duration_seconds'] = (timing['finished'] - timing['started']).total_seconds()\n",
    "    except Exception as exc:\n",
    "        print(f'Error loading timing for job {job_id}: {exc}')\n",
    "    return timing\n",
    "\n",
    "def analyze_single_run(job_id, config_label, project, base_path):\n",
    "    output_df = load_output_csv(job_id, base_path)\n",
    "    timing = load_timing_data(job_id, base_path)\n",
    "    if output_df is None or output_df.empty:\n",
    "        return None\n",
    "\n",
    "    initial_data = output_df[output_df['iteration'] == 0]\n",
    "    initial_numPass = int(initial_data.iloc[0]['numPass']) if len(initial_data) > 0 else 0\n",
    "\n",
    "    final_iteration = int(output_df['iteration'].max())\n",
    "    final_row = output_df[output_df['iteration'] == final_iteration].iloc[0]\n",
    "\n",
    "    total_evaluations = len(output_df)\n",
    "    actual_executions = int((output_df['fitnessEvalType'] == 'fitnessEvaluation').sum())\n",
    "    predictions = int((output_df['fitnessEvalType'] == 'fitnessPrediction').sum())\n",
    "\n",
    "    max_numPass = int(output_df['numPass'].max())\n",
    "    is_success = max_numPass == 28\n",
    "\n",
    "    optimal_iteration = None\n",
    "    if is_success:\n",
    "        opt_rows = output_df[output_df['numPass'] == 28]\n",
    "        if len(opt_rows) > 0:\n",
    "            optimal_iteration = int(opt_rows['iteration'].min())\n",
    "\n",
    "    first_improvement_iteration = None\n",
    "    improvement_rows = output_df[(output_df['numPass'] > initial_numPass) & (output_df['iteration'] > 0)]\n",
    "    if len(improvement_rows) > 0:\n",
    "        first_improvement_iteration = int(improvement_rows['iteration'].min())\n",
    "\n",
    "    final_fitness = float(output_df['fitness'].max())\n",
    "    target_fitness = 0.9 * final_fitness if final_fitness is not None else None\n",
    "    convergence_90_iteration = None\n",
    "    if target_fitness is not None and not np.isnan(target_fitness):\n",
    "        conv_rows = output_df[output_df['fitness'] >= target_fitness]\n",
    "        if len(conv_rows) > 0:\n",
    "            convergence_90_iteration = int(conv_rows['iteration'].min())\n",
    "\n",
    "    result = {\n",
    "        'job_id': int(job_id),\n",
    "        'config': config_label,\n",
    "        'project': project,\n",
    "        'initial_numPass': initial_numPass,\n",
    "        'is_success': bool(is_success),\n",
    "        'max_numPass': max_numPass,\n",
    "        'final_fitness': final_fitness,\n",
    "        'final_iteration': final_iteration,\n",
    "        'optimal_iteration': optimal_iteration,\n",
    "        'first_improvement_iteration': first_improvement_iteration,\n",
    "        'convergence_90_iteration': convergence_90_iteration,\n",
    "        'total_evaluations': int(total_evaluations),\n",
    "        'actual_executions': actual_executions,\n",
    "        'predictions': predictions,\n",
    "        'duration_seconds': timing.get('duration_seconds', np.nan),\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7160ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/410 tasks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/bgn55ct90rbfyh1dnn5gfq9m0000gn/T/ipykernel_90053/1272990764.py:20: DtypeWarning: Columns (3,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(output_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/410 tasks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/bgn55ct90rbfyh1dnn5gfq9m0000gn/T/ipykernel_90053/1272990764.py:20: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(output_file)\n",
      "/var/folders/ld/bgn55ct90rbfyh1dnn5gfq9m0000gn/T/ipykernel_90053/1272990764.py:20: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(output_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 150/410 tasks...\n",
      "Processed 200/410 tasks...\n",
      "Processed 250/410 tasks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/bgn55ct90rbfyh1dnn5gfq9m0000gn/T/ipykernel_90053/1272990764.py:20: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(output_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 300/410 tasks...\n",
      "Processed 350/410 tasks...\n",
      "Processed 400/410 tasks...\n",
      "Total analyzed runs: 410\n",
      "Saved: raw_data/per_run_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Process all runs and build per_run_results.csv\n",
    "results = []\n",
    "for idx, task in all_tasks.iterrows():\n",
    "    job_id = int(task['job_id'])\n",
    "    config_label = task['config_label']\n",
    "    project = task['project']\n",
    "    base_path = BASELINE_PATH if config_label == 'FI-00' else SURROGATE_PATH\n",
    "    row = analyze_single_run(job_id, config_label, project, base_path)\n",
    "    if row is not None:\n",
    "        results.append(row)\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f'Processed {idx + 1}/{len(all_tasks)} tasks...')\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f'Total analyzed runs: {len(results_df)}')\n",
    "\n",
    "# Derived columns used in downstream summaries\n",
    "results_df['prediction_rate'] = (results_df['predictions'] / results_df['total_evaluations'] * 100).fillna(0.0)\n",
    "\n",
    "# Approximate time to first improvement in seconds,\n",
    "# assuming uniform progress over the run duration.\n",
    "results_df['time_to_first_improvement_sec'] = np.where(\n",
    "    results_df['first_improvement_iteration'].notna() & results_df['duration_seconds'].notna(),\n",
    "    results_df['duration_seconds'] * (results_df['first_improvement_iteration'] / results_df['final_iteration'].replace(0, np.nan)),\n",
    "    np.nan,\n",
    ")\n",
    "\n",
    "results_df.to_csv(RAW_DATA_PATH / 'per_run_results.csv', index=False)\n",
    "print('Saved: raw_data/per_run_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2dbc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: raw_data/repair_success_summary.csv\n",
      "Saved: raw_data/time_efficiency_all_runs.csv\n",
      "Saved: raw_data/time_efficiency_successful_only.csv\n",
      "Saved: raw_data/time_to_first_partial_fix.csv\n",
      "Saved: raw_data/test_execution_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Core aggregated summaries by configuration\n",
    "\n",
    "config_group = results_df.groupby('config')\n",
    "\n",
    "# Repair success summary\n",
    "success_summary = config_group.agg({\n",
    "    'is_success': ['sum', 'count', 'mean'],\n",
    "    'max_numPass': ['mean', 'median', 'std'],\n",
    "    'final_fitness': ['mean', 'median', 'std'],\n",
    "}).round(4)\n",
    "success_summary.columns = ['_'.join(col).strip() for col in success_summary.columns.values]\n",
    "success_summary = success_summary.rename(columns={\n",
    "    'is_success_sum': 'successful_repairs',\n",
    "    'is_success_count': 'total_runs',\n",
    "    'is_success_mean': 'success_rate',\n",
    "    'max_numPass_mean': 'avg_tests_passed',\n",
    "    'max_numPass_median': 'median_tests_passed',\n",
    "    'max_numPass_std': 'std_tests_passed',\n",
    "    'final_fitness_mean': 'avg_final_fitness',\n",
    "    'final_fitness_median': 'median_final_fitness',\n",
    "    'final_fitness_std': 'std_final_fitness',\n",
    "})\n",
    "success_summary = success_summary.reindex(CONFIG_ORDER)\n",
    "success_summary.to_csv(RAW_DATA_PATH / 'repair_success_summary.csv')\n",
    "print('Saved: raw_data/repair_success_summary.csv')\n",
    "\n",
    "# Time efficiency (all runs and successful-only)\n",
    "time_summary = results_df[results_df['duration_seconds'].notna()].groupby('config').agg({\n",
    "    'duration_seconds': ['mean', 'median', 'std', 'min', 'max', 'count'],\n",
    "}).round(2)\n",
    "time_summary.columns = ['_'.join(col).strip() for col in time_summary.columns.values]\n",
    "time_summary = time_summary.rename(columns={\n",
    "    'duration_seconds_mean': 'mean_time_sec',\n",
    "    'duration_seconds_median': 'median_time_sec',\n",
    "    'duration_seconds_std': 'std_time_sec',\n",
    "    'duration_seconds_min': 'min_time_sec',\n",
    "    'duration_seconds_max': 'max_time_sec',\n",
    "    'duration_seconds_count': 'num_runs',\n",
    "})\n",
    "time_summary = time_summary.reindex(CONFIG_ORDER)\n",
    "\n",
    "baseline_mean = time_summary.loc['FI-00', 'mean_time_sec']\n",
    "time_summary['speedup_vs_baseline'] = baseline_mean / time_summary['mean_time_sec']\n",
    "time_summary['time_saved_pct'] = (1 - time_summary['mean_time_sec'] / baseline_mean) * 100\n",
    "\n",
    "time_summary.to_csv(RAW_DATA_PATH / 'time_efficiency_all_runs.csv')\n",
    "print('Saved: raw_data/time_efficiency_all_runs.csv')\n",
    "\n",
    "success_only = results_df[results_df['is_success'] & results_df['duration_seconds'].notna()]\n",
    "time_success = success_only.groupby('config').agg({\n",
    "    'duration_seconds': ['mean', 'median', 'std', 'min', 'max', 'count'],\n",
    "}).round(2)\n",
    "time_success.columns = ['_'.join(col).strip() for col in time_success.columns.values]\n",
    "time_success = time_success.rename(columns={\n",
    "    'duration_seconds_mean': 'mean_time_sec',\n",
    "    'duration_seconds_median': 'median_time_sec',\n",
    "    'duration_seconds_std': 'std_time_sec',\n",
    "    'duration_seconds_min': 'min_time_sec',\n",
    "    'duration_seconds_max': 'max_time_sec',\n",
    "    'duration_seconds_count': 'num_runs',\n",
    "})\n",
    "time_success = time_success.reindex(CONFIG_ORDER)\n",
    "\n",
    "baseline_mean_success = time_success.loc['FI-00', 'mean_time_sec']\n",
    "time_success['speedup_vs_baseline'] = baseline_mean_success / time_success['mean_time_sec']\n",
    "time_success['time_saved_pct'] = (1 - time_success['mean_time_sec'] / baseline_mean_success) * 100\n",
    "\n",
    "time_success.to_csv(RAW_DATA_PATH / 'time_efficiency_successful_only.csv')\n",
    "print('Saved: raw_data/time_efficiency_successful_only.csv')\n",
    "\n",
    "# Additional aggregated summaries (first improvement, test executions, etc.)\n",
    "first_improvement = results_df.groupby('config').agg({\n",
    "    'first_improvement_iteration': ['mean', 'median', 'std', 'min', 'max', 'count'],\n",
    "    'time_to_first_improvement_sec': ['mean', 'median'],\n",
    "}).round(2)\n",
    "first_improvement.columns = ['_'.join(col).strip() for col in first_improvement.columns.values]\n",
    "first_improvement = first_improvement.rename(columns={\n",
    "    'first_improvement_iteration_mean': 'mean_first_improvement_iter',\n",
    "    'first_improvement_iteration_median': 'median_first_improvement_iter',\n",
    "    'first_improvement_iteration_std': 'std_first_improvement_iter',\n",
    "    'first_improvement_iteration_min': 'min_first_improvement_iter',\n",
    "    'first_improvement_iteration_max': 'max_first_improvement_iter',\n",
    "    'first_improvement_iteration_count': 'num_runs_with_improvement',\n",
    "    'time_to_first_improvement_sec_mean': 'mean_time_to_first_improvement_sec',\n",
    "    'time_to_first_improvement_sec_median': 'median_time_to_first_improvement_sec',\n",
    "})\n",
    "first_improvement = first_improvement.reindex(CONFIG_ORDER)\n",
    "first_improvement.to_csv(RAW_DATA_PATH / 'time_to_first_partial_fix.csv')\n",
    "print('Saved: raw_data/time_to_first_partial_fix.csv')\n",
    "\n",
    "# Test execution summary\n",
    "test_exec_summary = results_df.groupby('config').agg({\n",
    "    'actual_executions': ['mean', 'median', 'std', 'min', 'max', 'count'],\n",
    "}).round(2)\n",
    "test_exec_summary.columns = ['_'.join(col).strip() for col in test_exec_summary.columns.values]\n",
    "test_exec_summary = test_exec_summary.rename(columns={\n",
    "    'actual_executions_mean': 'mean_test_executions',\n",
    "    'actual_executions_median': 'median_test_executions',\n",
    "    'actual_executions_std': 'std_test_executions',\n",
    "    'actual_executions_min': 'min_test_executions',\n",
    "    'actual_executions_max': 'max_test_executions',\n",
    "    'actual_executions_count': 'num_runs',\n",
    "})\n",
    "test_exec_summary = test_exec_summary.reindex(CONFIG_ORDER)\n",
    "test_exec_summary.to_csv(RAW_DATA_PATH / 'test_execution_summary.csv')\n",
    "print('Saved: raw_data/test_execution_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b26f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: raw_data/partial_fixes_summary.csv\n",
      "Saved: raw_data/partial_fixes_speed_comparison.csv\n",
      "Saved: raw_data/partial_fixes_quality_distribution.csv\n",
      "Saved: raw_data/partial_fixes_quality_percentages.csv\n",
      "Saved: raw_data/partial_fixes_time_quality_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Partial fixes summaries and quality metrics\n",
    "\n",
    "# Runs with any improvement (partial or full)\n",
    "improved_runs = results_df[results_df['first_improvement_iteration'].notna()].copy()\n",
    "\n",
    "# Project-level best improvement\n",
    "project_best = improved_runs.sort_values(\n",
    "    ['project', 'config', 'max_numPass'], ascending=[True, True, False]\n",
    ").drop_duplicates(subset=['project', 'config'])\n",
    "\n",
    "projects_with_improvement = project_best.groupby('config')['project'].nunique()\n",
    "projects_total = results_df.groupby('config')['project'].nunique()\n",
    "partial_fixes_summary = project_best.groupby('config').agg({\n",
    "    'first_improvement_iteration': ['mean', 'median'],\n",
    "    'max_numPass': ['mean', 'median'],\n",
    "}).round(2)\n",
    "partial_fixes_summary.columns = ['_'.join(col).strip() for col in partial_fixes_summary.columns.values]\n",
    "partial_fixes_summary = partial_fixes_summary.rename(columns={\n",
    "    'first_improvement_iteration_mean': 'mean_iterations_to_fix',\n",
    "    'first_improvement_iteration_median': 'median_iterations_to_fix',\n",
    "    'max_numPass_mean': 'avg_tests_passed',\n",
    "    'max_numPass_median': 'median_tests_passed',\n",
    "})\n",
    "partial_fixes_summary['projects_with_improvement'] = projects_with_improvement\n",
    "partial_fixes_summary['total_projects'] = projects_total\n",
    "partial_fixes_summary['improvement_rate'] = (\n",
    "    partial_fixes_summary['projects_with_improvement'] / partial_fixes_summary['total_projects']\n",
    ")\n",
    "partial_fixes_summary = partial_fixes_summary.reindex(CONFIG_ORDER)\n",
    "partial_fixes_summary.to_csv(RAW_DATA_PATH / 'partial_fixes_summary.csv')\n",
    "print('Saved: raw_data/partial_fixes_summary.csv')\n",
    "\n",
    "# Speed comparison for partial fixes\n",
    "if not improved_runs.empty:\n",
    "    speed_comparison = partial_fixes_summary[[\n",
    "        'median_iterations_to_fix',\n",
    "        'mean_iterations_to_fix',\n",
    "        'avg_tests_passed',\n",
    "    ]].copy()\n",
    "    speed_comparison['count_improvements'] = improved_runs.groupby('config').size()\n",
    "    speed_comparison = speed_comparison.sort_values('median_iterations_to_fix')\n",
    "    speed_comparison.to_csv(RAW_DATA_PATH / 'partial_fixes_speed_comparison.csv')\n",
    "    print('Saved: raw_data/partial_fixes_speed_comparison.csv')\n",
    "else:\n",
    "    print('No improved runs; skipping partial_fixes_speed_comparison.csv')\n",
    "\n",
    "# Partial fix quality distribution\n",
    "def categorize_improvement(num_pass, total_tests=28):\n",
    "    if num_pass == total_tests:\n",
    "        return 'Full Fix'\n",
    "    elif num_pass >= 0.75 * total_tests:\n",
    "        return 'High Quality (75%+)'\n",
    "    elif num_pass >= 0.5 * total_tests:\n",
    "        return 'Medium Quality (50-75%)'\n",
    "    elif num_pass >= 0.25 * total_tests:\n",
    "        return 'Low Quality (25-50%)'\n",
    "    else:\n",
    "        return 'Minimal (<25%)'\n",
    "\n",
    "if not improved_runs.empty:\n",
    "    improved_runs['quality_category'] = improved_runs['max_numPass'].apply(categorize_improvement)\n",
    "    quality_distribution = improved_runs.groupby(['config', 'quality_category']).size().unstack(fill_value=0)\n",
    "    categories = [\n",
    "        'Minimal (<25%)',\n",
    "        'Low Quality (25-50%)',\n",
    "        'Medium Quality (50-75%)',\n",
    "        'High Quality (75%+)',\n",
    "        'Full Fix',\n",
    "    ]\n",
    "    for cat in categories:\n",
    "        if cat not in quality_distribution.columns:\n",
    "            quality_distribution[cat] = 0\n",
    "    quality_distribution = quality_distribution[categories]\n",
    "    quality_distribution['Total'] = quality_distribution.sum(axis=1)\n",
    "    quality_distribution = quality_distribution.reindex(CONFIG_ORDER, fill_value=0)\n",
    "    quality_percentages = quality_distribution.div(quality_distribution['Total'], axis=0) * 100\n",
    "    quality_percentages = quality_percentages.drop('Total', axis=1).round(1)\n",
    "    quality_distribution.to_csv(RAW_DATA_PATH / 'partial_fixes_quality_distribution.csv')\n",
    "    quality_percentages.to_csv(RAW_DATA_PATH / 'partial_fixes_quality_percentages.csv')\n",
    "    print('Saved: raw_data/partial_fixes_quality_distribution.csv')\n",
    "    print('Saved: raw_data/partial_fixes_quality_percentages.csv')\n",
    "    # Combined time and quality metrics (simple join of summaries)\n",
    "    combined_metrics = partial_fixes_summary.join(quality_percentages, how='left')\n",
    "    combined_metrics.to_csv(RAW_DATA_PATH / 'partial_fixes_time_quality_combined.csv')\n",
    "    print('Saved: raw_data/partial_fixes_time_quality_combined.csv')\n",
    "else:\n",
    "    print('No improved runs; skipping quality distribution CSVs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b617683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: raw_data/partial_fixes_statistical_tests.csv\n",
      "Saved: raw_data/statistical_tests.csv\n",
      "Saved: raw_data/statistical_tests_additional.csv\n"
     ]
    }
   ],
   "source": [
    "# Statistical tests between configurations\n",
    "\n",
    "comparisons = [\n",
    "    ('FI-00', 'FI-50', 'Baseline vs FI-50%'),\n",
    "    ('FI-00', 'FI-70', 'Baseline vs FI-70%'),\n",
    "    ('FI-00', 'TO-50', 'Baseline vs TO-50%'),\n",
    "    ('FI-00', 'TO-70', 'Baseline vs TO-70%'),\n",
    "    ('FI-50', 'FI-70', 'FI-50% vs FI-70%'),\n",
    "    ('TO-50', 'TO-70', 'TO-50% vs TO-70%'),\n",
    "    ('FI-50', 'TO-50', 'FI vs TO (50%)'),\n",
    "    ('FI-70', 'TO-70', 'FI vs TO (70%)'),\n",
    "]\n",
    "\n",
    "# Partial fix statistical tests\n",
    "partial_stats_results = []\n",
    "for config1, config2, label in comparisons:\n",
    "    data1 = results_df[results_df['config'] == config1]\n",
    "    data2 = results_df[results_df['config'] == config2]\n",
    "\n",
    "    improved1 = data1['first_improvement_iteration'].notna().sum()\n",
    "    total1 = len(data1)\n",
    "    improved2 = data2['first_improvement_iteration'].notna().sum()\n",
    "    total2 = len(data2)\n",
    "\n",
    "    contingency = np.array([[improved1, total1 - improved1],\n",
    "                            [improved2, total2 - improved2]])\n",
    "    try:\n",
    "        chi2_impr, p_impr = stats.chi2_contingency(contingency)[:2]\n",
    "    except ValueError:\n",
    "        chi2_impr, p_impr = np.nan, np.nan\n",
    "\n",
    "    iter1 = data1[data1['first_improvement_iteration'].notna()]['first_improvement_iteration']\n",
    "    iter2 = data2[data2['first_improvement_iteration'].notna()]['first_improvement_iteration']\n",
    "    if len(iter1) > 0 and len(iter2) > 0:\n",
    "        u_iter, p_iter = stats.mannwhitneyu(iter1, iter2, alternative='two-sided')\n",
    "        eff_iter = cohens_d(iter1, iter2)\n",
    "    else:\n",
    "        u_iter, p_iter, eff_iter = np.nan, np.nan, np.nan\n",
    "\n",
    "    tests1 = data1['max_numPass']\n",
    "    tests2 = data2['max_numPass']\n",
    "    if len(tests1) > 0 and len(tests2) > 0:\n",
    "        u_tests, p_tests = stats.mannwhitneyu(tests1, tests2, alternative='two-sided')\n",
    "        eff_tests = cohens_d(tests1, tests2)\n",
    "    else:\n",
    "        u_tests, p_tests, eff_tests = np.nan, np.nan, np.nan\n",
    "\n",
    "    partial_stats_results.append({\n",
    "        'comparison': label,\n",
    "        'config1': config1,\n",
    "        'config2': config2,\n",
    "        'chi2_improvement': chi2_impr,\n",
    "        'p_value_improvement_rate': p_impr,\n",
    "        'improvement_rate_1': improved1 / total1 if total1 > 0 else np.nan,\n",
    "        'improvement_rate_2': improved2 / total2 if total2 > 0 else np.nan,\n",
    "        'p_value_iterations': p_iter,\n",
    "        'effect_size_iterations': eff_iter,\n",
    "        'mean_iterations_1': iter1.mean() if len(iter1) > 0 else np.nan,\n",
    "        'mean_iterations_2': iter2.mean() if len(iter2) > 0 else np.nan,\n",
    "        'p_value_tests_passed': p_tests,\n",
    "        'effect_size_tests_passed': eff_tests,\n",
    "        'mean_tests_passed_1': tests1.mean() if len(tests1) > 0 else np.nan,\n",
    "        'mean_tests_passed_2': tests2.mean() if len(tests2) > 0 else np.nan,\n",
    "    })\n",
    "\n",
    "partial_stats_df = pd.DataFrame(partial_stats_results).round(4)\n",
    "partial_stats_df.to_csv(RAW_DATA_PATH / 'partial_fixes_statistical_tests.csv', index=False)\n",
    "print('Saved: raw_data/partial_fixes_statistical_tests.csv')\n",
    "\n",
    "# Main statistical tests for time and test executions (vs baseline and others)\n",
    "statistical_results = []\n",
    "for config1, config2, label in comparisons:\n",
    "    data1 = results_df[results_df['config'] == config1]\n",
    "    data2 = results_df[results_df['config'] == config2]\n",
    "\n",
    "    success1 = data1['is_success'].sum()\n",
    "    total1 = len(data1)\n",
    "    success2 = data2['is_success'].sum()\n",
    "    total2 = len(data2)\n",
    "    if success1 + success2 >= 5:\n",
    "        contingency_success = np.array([[success1, total1 - success1],\n",
    "                                        [success2, total2 - success2]])\n",
    "        try:\n",
    "            chi2_succ, p_succ = stats.chi2_contingency(contingency_success)[:2]\n",
    "        except ValueError:\n",
    "            chi2_succ, p_succ = np.nan, np.nan\n",
    "    else:\n",
    "        chi2_succ, p_succ = np.nan, np.nan\n",
    "\n",
    "    time1 = data1[data1['duration_seconds'].notna()]['duration_seconds']\n",
    "    time2 = data2[data2['duration_seconds'].notna()]['duration_seconds']\n",
    "    if len(time1) > 0 and len(time2) > 0:\n",
    "        u_time, p_time = stats.mannwhitneyu(time1, time2, alternative='two-sided')\n",
    "        eff_time = cohens_d(time1, time2)\n",
    "    else:\n",
    "        u_time, p_time, eff_time = np.nan, np.nan, np.nan\n",
    "\n",
    "    exec1 = data1['actual_executions']\n",
    "    exec2 = data2['actual_executions']\n",
    "    if len(exec1) > 0 and len(exec2) > 0:\n",
    "        u_exec, p_exec = stats.mannwhitneyu(exec1, exec2, alternative='two-sided')\n",
    "        eff_exec = cohens_d(exec1, exec2)\n",
    "    else:\n",
    "        u_exec, p_exec, eff_exec = np.nan, np.nan, np.nan\n",
    "\n",
    "    statistical_results.append({\n",
    "        'comparison': label,\n",
    "        'config1': config1,\n",
    "        'config2': config2,\n",
    "        'chi2_success': chi2_succ,\n",
    "        'p_value_success': p_succ,\n",
    "        'success_rate_1': success1 / total1 if total1 > 0 else np.nan,\n",
    "        'success_rate_2': success2 / total2 if total2 > 0 else np.nan,\n",
    "        'p_value_time': p_time,\n",
    "        'effect_size_time': eff_time,\n",
    "        'mean_time_1': time1.mean() if len(time1) > 0 else np.nan,\n",
    "        'mean_time_2': time2.mean() if len(time2) > 0 else np.nan,\n",
    "        'p_value_executions': p_exec,\n",
    "        'effect_size_executions': eff_exec,\n",
    "        'mean_exec_1': exec1.mean() if len(exec1) > 0 else np.nan,\n",
    "        'mean_exec_2': exec2.mean() if len(exec2) > 0 else np.nan,\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(statistical_results).round(4)\n",
    "stats_df.to_csv(RAW_DATA_PATH / 'statistical_tests.csv', index=False)\n",
    "print('Saved: raw_data/statistical_tests.csv')\n",
    "\n",
    "# Additional statistical tests for time to first improvement and generations\n",
    "additional_stats = []\n",
    "if 'time_to_first_partial_fix.csv' in [p.name for p in RAW_DATA_PATH.glob('*.csv')]:\n",
    "    time_first_df = pd.read_csv(RAW_DATA_PATH / 'time_to_first_partial_fix.csv')\n",
    "else:\n",
    "    time_first_df = None\n",
    "\n",
    "for config1, config2, label in comparisons:\n",
    "    data1 = results_df[results_df['config'] == config1]\n",
    "    data2 = results_df[results_df['config'] == config2]\n",
    "\n",
    "    if time_first_df is not None:\n",
    "        row1 = time_first_df[time_first_df['config'] == config1]\n",
    "        row2 = time_first_df[time_first_df['config'] == config2]\n",
    "        median_time1 = row1['median_time_to_first_improvement_sec'].values[0] if len(row1) > 0 else np.nan\n",
    "        median_time2 = row2['median_time_to_first_improvement_sec'].values[0] if len(row2) > 0 else np.nan\n",
    "        mean_time1 = row1['mean_time_to_first_improvement_sec'].values[0] if len(row1) > 0 else np.nan\n",
    "        mean_time2 = row2['mean_time_to_first_improvement_sec'].values[0] if len(row2) > 0 else np.nan\n",
    "        mean_time1 = row1['mean_time_to_first_improvement_sec'].values[0] if len(row1) > 0 else np.nan\n",
    "        mean_time2 = row2['mean_time_to_first_improvement_sec'].values[0] if len(row2) > 0 else np.nan\n",
    "    else:\n",
    "        median_time1 = median_time2 = mean_time1 = mean_time2 = np.nan\n",
    "\n",
    "    improved1 = data1[data1['first_improvement_iteration'].notna()]['first_improvement_iteration']\n",
    "    improved2 = data2[data2['first_improvement_iteration'].notna()]['first_improvement_iteration']\n",
    "    if len(improved1) > 5 and len(improved2) > 5:\n",
    "        u_time_first, p_time_first = stats.mannwhitneyu(improved1, improved2, alternative='two-sided')\n",
    "        eff_time_first = cohens_d(improved1, improved2)\n",
    "    else:\n",
    "        u_time_first, p_time_first, eff_time_first = np.nan, np.nan, np.nan\n",
    "\n",
    "    gen1 = data1['final_iteration']\n",
    "    gen2 = data2['final_iteration']\n",
    "    if len(gen1) > 0 and len(gen2) > 0:\n",
    "        u_gen, p_gen = stats.mannwhitneyu(gen1, gen2, alternative='two-sided')\n",
    "        eff_gen = cohens_d(gen1, gen2)\n",
    "        mean_gen1 = gen1.mean()\n",
    "        mean_gen2 = gen2.mean()\n",
    "        median_gen1 = gen1.median()\n",
    "        median_gen2 = gen2.median()\n",
    "    else:\n",
    "        u_gen, p_gen, eff_gen = np.nan, np.nan, np.nan\n",
    "        mean_gen1 = mean_gen2 = median_gen1 = median_gen2 = np.nan\n",
    "\n",
    "    additional_stats.append({\n",
    "        'comparison': label,\n",
    "        'config1': config1,\n",
    "        'config2': config2,\n",
    "        'p_value_time_first_improvement': p_time_first,\n",
    "        'effect_size_time_first_improvement': eff_time_first,\n",
    "        'median_time_first_imp_sec_1': median_time1,\n",
    "        'median_time_first_imp_sec_2': median_time2,\n",
    "        'mean_time_first_imp_sec_1': mean_time1,\n",
    "        'mean_time_first_imp_sec_2': mean_time2,\n",
    "        'p_value_generations': p_gen,\n",
    "        'effect_size_generations': eff_gen,\n",
    "        'mean_generations_1': mean_gen1,\n",
    "        'mean_generations_2': mean_gen2,\n",
    "        'median_generations_1': median_gen1,\n",
    "        'median_generations_2': median_gen2,\n",
    "    })\n",
    "\n",
    "additional_stats_df = pd.DataFrame(additional_stats).round(4)\n",
    "additional_stats_df.to_csv(RAW_DATA_PATH / 'statistical_tests_additional.csv', index=False)\n",
    "print('Saved: raw_data/statistical_tests_additional.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggnn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
